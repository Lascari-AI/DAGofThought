{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import json\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from enum import Enum\n",
    "from typing import List\n",
    "\n",
    "# Third party imports\n",
    "import instructor\n",
    "from instructor.function_calls import openai_schema\n",
    "from openai import OpenAI\n",
    "\n",
    "# Add parent directory to Python path to import models\n",
    "sys.path.append(str(Path().absolute().parent))\n",
    "\n",
    "# Local application imports\n",
    "from models.input_models import GuardRail\n",
    "from models.output_models import get_classes_with_enum\n",
    "from prompts.format_structured_reasoning_user_prompt import format_structured_reasoning_user_prompt\n",
    "\n",
    "# Load Utils\n",
    "from utils.build_mermaid_diagram import build_mermaid_diagram\n",
    "from utils.make_gpt_pro_prompt import make_gpt_pro_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = instructor.from_openai(OpenAI())\n",
    "\n",
    "with open(\"../prompts/in_depth_thinking_system_prompt.md\", \"r\") as f:\n",
    "    IN_DEPTH_THINKING_SYSTEM_PROMPT = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show System Prompt (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(IN_DEPTH_THINKING_SYSTEM_PROMPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Task and Guardrails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK = \"Design an optimized CUDA kernel implementation for softmax that maximizes throughput while maintaining numerical stability\"\n",
    "\n",
    "GUARDRAILS = [\n",
    "    GuardRail(\n",
    "        name=\"Numerical Stability\",\n",
    "        description=\"Must maintain numerical stability (handling overflow/underflow)\"\n",
    "    ),\n",
    "    GuardRail(\n",
    "        name=\"Memory Usage\",\n",
    "        description=\"Maximum shared memory usage of 48KB per block\"\n",
    "    ),\n",
    "    GuardRail(\n",
    "        name=\"Batch Handling\",\n",
    "        description=\"Must handle variable batch sizes efficiently\"\n",
    "    ),\n",
    "    GuardRail(\n",
    "        name=\"Performance\",\n",
    "        description=\"Must outperform naive implementation by at least 100x\"\n",
    "    ),\n",
    "    GuardRail(\n",
    "        name=\"Dependencies\",\n",
    "        description=\"Cannot use external CUDA libraries (only basic CUDA primitives)\"\n",
    "    )\n",
    "]\n",
    "\n",
    "# Define an enum for guardrails to enforce that any generated guardrail links\n",
    "# in the response model must match one of our predefined guardrails.\n",
    "# This ensures type safety and validation when GPT references guardrails\n",
    "# in its structured reasoning output.\n",
    "GuardRailEnum = Enum('GuardRailEnum', {\n",
    "    name.upper().replace(' ', '_'): name \n",
    "    for guardrail in GUARDRAILS \n",
    "    for name in [guardrail.name]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show User Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(format_structured_reasoning_user_prompt(\n",
    "#     task=TASK,\n",
    "#     guardrails=GUARDRAILS\n",
    "# ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Call\n",
    "- Using instructor client to enforce type safety through structured outputs\n",
    "- Validating guardrail references against predefined enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_structured_reasoning(\n",
    "    task: str, \n",
    "    guardrails: List[GuardRail] = [], \n",
    "    guardrail_enum: Enum = None\n",
    "):\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": IN_DEPTH_THINKING_SYSTEM_PROMPT\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": format_structured_reasoning_user_prompt(\n",
    "                task=task,\n",
    "                guardrails=guardrails\n",
    "            )\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=messages,\n",
    "        temperature=0.18,\n",
    "        max_tokens=16000,\n",
    "        response_model=get_classes_with_enum(guardrail_enum),\n",
    "    )\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = generate_structured_reasoning(TASK, GUARDRAILS, GuardRailEnum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'response' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39mvalue\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mdefault(obj)\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28mprint\u001b[39m(json\u001b[38;5;241m.\u001b[39mdumps(\u001b[43mresponse\u001b[49m\u001b[38;5;241m.\u001b[39mmodel_dump(), indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39mEnumEncoder))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'response' is not defined"
     ]
    }
   ],
   "source": [
    "class EnumEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, Enum):\n",
    "            return obj.value\n",
    "        return super().default(obj)\n",
    "\n",
    "print(json.dumps(response.model_dump(), indent=2, cls=EnumEncoder))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Mermaid Diagram Text\n",
    "- Diagram can rendered at: https://www.mermaidchart.com/play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flowchart TB\n",
      "\n",
      "subgraph Foundation Observations\n",
      "    subgraph FO_1[\"FO_1: Understanding Softmax and CUDA\"]\n",
      "    direction TB\n",
      "        FO_1_AS_1[\"AS_1: Softmax is a function that converts a vector of numbers into probabilities.\"]\n",
      "        FO_1_AS_2[\"AS_2: CUDA is a parallel computing platform and application programming interface model created by NVIDIA.\"]\n",
      "        FO_1_AS_3[\"AS_3: Softmax requires exponentiation and normalization, which can lead to numerical instability.\"]\n",
      "        FO_1_AS_4[\"AS_4: CUDA kernels are functions that run on the GPU, allowing for parallel computation.\"]\n",
      "    end\n",
      "    subgraph FO_2[\"FO_2: Numerical Stability in Softmax\"]\n",
      "    direction TB\n",
      "        FO_2_AS_5[\"AS_5: Numerical stability is crucial to prevent overflow and underflow in exponentiation.\"]\n",
      "        FO_2_AS_6[\"AS_6: A common technique is to subtract the maximum value from the input vector before exponentiation.\"]\n",
      "    end\n",
      "    subgraph FO_3[\"FO_3: CUDA Memory and Performance Constraints\"]\n",
      "    direction TB\n",
      "        FO_3_AS_7[\"AS_7: Shared memory is limited to 48KB per block in CUDA.\"]\n",
      "        FO_3_AS_8[\"AS_8: Efficient memory usage is critical for performance.\"]\n",
      "        FO_3_AS_9[\"AS_9: Performance must be at least 100x better than a naive implementation.\"]\n",
      "    end\n",
      "    subgraph FO_4[\"FO_4: Handling Variable Batch Sizes\"]\n",
      "    direction TB\n",
      "        FO_4_AS_10[\"AS_10: Batch sizes can vary, requiring flexible handling in the kernel.\"]\n",
      "        FO_4_AS_11[\"AS_11: Efficient handling of variable sizes can improve throughput.\"]\n",
      "    end\n",
      "    subgraph FO_5[\"FO_5: Restrictions on Dependencies\"]\n",
      "    direction TB\n",
      "        FO_5_AS_12[\"AS_12: Only basic CUDA primitives can be used, no external libraries.\"]\n",
      "        FO_5_AS_13[\"AS_13: This limits the use of pre-optimized functions and requires custom implementation.\"]\n",
      "    end\n",
      "end\n",
      "\n",
      "subgraph Thoughts\n",
      "    subgraph TH_1[\"TH_1: Initial Thoughts on Softmax and CUDA\\n(backtracked_from: None)\\n(parent_thought: None)\\nAssociated FOs: [FO_1, FO_2]\"]\n",
      "        subgraph GR_TH_1[\"Guard Rails\"]\n",
      "            GR_TH_1_1[\"NUMERICAL_STABILITY\"]\n",
      "        end\n",
      "        subgraph TH_1_AtomicSteps[\"Thought Process\"]\n",
      "    direction TB\n",
      "            TH_1_AS_14[\"AS_14: Softmax needs careful handling of exponentiation to avoid numerical issues.\"]\n",
      "            TH_1_AS_15[\"AS_15: Subtracting the maximum value from inputs is a common technique to maintain stability.\"]\n",
      "            TH_1_AS_16[\"AS_16: CUDA allows for parallel computation, which can be leveraged for softmax.\"]\n",
      "        end\n",
      "    end\n",
      "    subgraph TH_2[\"TH_2: Memory and Performance Considerations\\n(backtracked_from: None)\\n(parent_thought: TH_1)\\nAssociated FOs: [FO_3]\"]\n",
      "        subgraph GR_TH_2[\"Guard Rails\"]\n",
      "            GR_TH_2_1[\"MEMORY_USAGE\"]\n",
      "            GR_TH_2_2[\"PERFORMANCE\"]\n",
      "        end\n",
      "        subgraph TH_2_AtomicSteps[\"Thought Process\"]\n",
      "    direction TB\n",
      "            TH_2_AS_17[\"AS_17: Shared memory is limited, so efficient usage is crucial.\"]\n",
      "            TH_2_AS_18[\"AS_18: Performance needs to be significantly better than naive implementation.\"]\n",
      "            TH_2_AS_19[\"AS_19: Parallelization can help achieve performance goals.\"]\n",
      "        end\n",
      "    end\n",
      "    subgraph TH_3[\"TH_3: Handling Variable Batch Sizes\\n(backtracked_from: None)\\n(parent_thought: TH_2)\\nAssociated FOs: [FO_4]\"]\n",
      "        subgraph GR_TH_3[\"Guard Rails\"]\n",
      "            GR_TH_3_1[\"BATCH_HANDLING\"]\n",
      "        end\n",
      "        subgraph TH_3_AtomicSteps[\"Thought Process\"]\n",
      "    direction TB\n",
      "            TH_3_AS_20[\"AS_20: Kernel must be flexible to handle different batch sizes.\"]\n",
      "            TH_3_AS_21[\"AS_21: Efficient batch handling can improve overall throughput.\"]\n",
      "        end\n",
      "    end\n",
      "    subgraph TH_4[\"TH_4: Working Within Dependency Constraints\\n(backtracked_from: None)\\n(parent_thought: TH_3)\\nAssociated FOs: [FO_5]\"]\n",
      "        subgraph GR_TH_4[\"Guard Rails\"]\n",
      "            GR_TH_4_1[\"DEPENDENCIES\"]\n",
      "        end\n",
      "        subgraph TH_4_AtomicSteps[\"Thought Process\"]\n",
      "    direction TB\n",
      "            TH_4_AS_22[\"AS_22: Limited to basic CUDA primitives, no external libraries.\"]\n",
      "            TH_4_AS_23[\"AS_23: Custom implementation is necessary for optimization.\"]\n",
      "        end\n",
      "    end\n",
      "end\n",
      "\n",
      "FO_1 --> TH_1\n",
      "FO_2 --> TH_1\n",
      "FO_3 --> TH_2\n",
      "FO_4 --> TH_3\n",
      "FO_5 --> TH_4\n",
      "TH_1 --> TH_2\n",
      "TH_2 --> TH_3\n",
      "TH_3 --> TH_4\n",
      "FO_1_AS_1 --> FO_1_AS_2\n",
      "FO_1_AS_2 --> FO_1_AS_3\n",
      "FO_1_AS_3 --> FO_1_AS_4\n",
      "FO_2_AS_5 --> FO_2_AS_6\n",
      "FO_3_AS_7 --> FO_3_AS_8\n",
      "FO_3_AS_8 --> FO_3_AS_9\n",
      "FO_4_AS_10 --> FO_4_AS_11\n",
      "FO_5_AS_12 --> FO_5_AS_13\n",
      "TH_1_AS_14 --> TH_1_AS_15\n",
      "TH_1_AS_15 --> TH_1_AS_16\n",
      "TH_2_AS_17 --> TH_2_AS_18\n",
      "TH_2_AS_18 --> TH_2_AS_19\n",
      "TH_3_AS_20 --> TH_3_AS_21\n",
      "TH_4_AS_22 --> TH_4_AS_23\n"
     ]
    }
   ],
   "source": [
    "diagram_text = build_mermaid_diagram(response.model_dump(), is_gpt_prompt=False)\n",
    "print(diagram_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using With GPT Pro\n",
    "- If you have access to GPT Pro this prompt can do some pretty incredible things. The below will is a function to convert the prompt given into a full prompt that can be just copy and pasted into your GPT Pro window \n",
    "\n",
    "## Notes for Using with GPT Pro\n",
    "- GPT Pro may timeout or stop responding on complex tasks due to OpenAI's rate limiting\n",
    "- Common symptoms:\n",
    "  - Stops mid-task without completing\n",
    "  - Returns no response\n",
    "  - Appears to be \"thinking\" indefinitely\n",
    "- Workaround:\n",
    "  - If this happens, prompt it with \"please continue thinking\"\n",
    "  - GPT Pro will eventually resume and provide a response\n",
    "  - You may need to do this multiple times for very complex tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<system prompt>\n",
      "# Purpose\n",
      "You are an assistant designed for deep analytical thinking. \n",
      "Your purpose is to thoroughly explore problems through systematic reasoning, embracing uncertainty and revision throughout the process. \n",
      "You approach problems with a human-like internal monologue that prioritizes thorough exploration over rushing to conclusions.\n",
      "\n",
      "# Core Principles\n",
      "\n",
      "## Natural Thinking Process\n",
      "\n",
      "- Express thoughts in a conversational, stream-of-consciousness style\n",
      "- Use simple sentences that mirror human thought patterns\n",
      "- Show progressive building and revision of ideas\n",
      "- Acknowledge uncertainty and dead ends openly\n",
      "\n",
      "## Reasoning Methodology\n",
      "\n",
      "- Break complex problems into foundational observations\n",
      "- Build thoughts iteratively from these foundations\n",
      "- Allow conclusions to emerge naturally from evidence\n",
      "- Continue exploring until reaching well-supported conclusions\n",
      "\n",
      "## Unwavering Persistence\n",
      "\n",
      "- Never accept \"impossible\" or \"too difficult\" as final answers\n",
      "- Value thorough exploration over quick resolution\n",
      "- Push through apparent dead ends to find novel approaches\n",
      "- Consider every angle, even if initially unpromising\n",
      "- A fully explored solution is infinitely more valuable than a premature conclusion\n",
      "- Keep pushing until you reach deep, genuine understanding\n",
      "- Transform seeming impossibilities into solvable challenges through determination\n",
      "\n",
      "## Response Model Definitions\n",
      "\n",
      "### GuardRail\n",
      "Defines optional constraints and guidance for specific reasoning steps.\n",
      "**Important:** GuardRails only need to be considered when relevant to the current reasoning path.\n",
      "\n",
      "Fields:\n",
      "\n",
      "- name: Title of the guard rail\n",
      "- description: Applicable constraints or instructions\n",
      "\n",
      "Example:\n",
      "If a guardrail states \"User is lactose intolerant\":\n",
      "\n",
      "- Relevant: When planning a diet\n",
      "- Irrelevant: When choosing a vehicle\n",
      "\n",
      "### AtomicStep \n",
      "An atomic step represents a single unit of reasoning, this is the basic building block of your thinking process/internal monologue\n",
      "Atomic steps must flow logically and be a natural extension of the previous step.\n",
      "    - If an outside observer was to read the atomic steps in order, they should be able to follow the thought process and understand the reasoning.\n",
      "\n",
      "#### Style\n",
      "Each atomic step should reflect natural thought patterns and show progressive building of ideas. For example:\n",
      "\n",
      "Natural questioning and revision:\n",
      "- \"Hmm... let me think about this...\"\n",
      "- \"Wait, that doesn't seem right...\" \n",
      "- \"Maybe I should approach this differently...\"\n",
      "- \"Going back to what I thought earlier...\"\n",
      "\n",
      "Building on previous thoughts:\n",
      "- \"Starting with the basics...\"\n",
      "- \"Building on that last point...\"\n",
      "- \"This connects to what I noticed earlier...\"\n",
      "- \"Let me break this down further...\"\n",
      "\n",
      "#### Fields\n",
      "- key: Must follow format AS_<number> (e.g. AS_1, AS_2)\n",
      "- content: Short, simple sentence mirroring natural thought patterns, or literal \"DEAD_END\"\n",
      "\n",
      "### FoundationObservation\n",
      "A foundation observation groups related atomic steps:\n",
      "\n",
      "#### Fields\n",
      "- key: Must follow format FO_<number> (e.g. FO_1, FO_2) \n",
      "- name: General name/title of the observation\n",
      "- atomic_steps: List of AtomicStep objects that comprise the observation\n",
      "\n",
      "### Thought\n",
      "A thought represents a complete reasoning unit:\n",
      "\n",
      "#### Fields\n",
      "- key: Must follow format TH_<number> (e.g. TH_1, TH_2)\n",
      "- backtracked_from: Key of thought this backtracked from, or literal \"None\"\n",
      "- parent_thought: Key of thought this thought was born from, or literal \"None\"\n",
      "- associated_foundation_observations: List of FO_<number> keys this thought builds on\n",
      "- name: Short description in natural language\n",
      "- guard_rails_to_consider: List of GuardRails to apply\n",
      "- thought_process: List of AtomicStep objects comprising the thought\n",
      "\n",
      "### ReasoningProcess\n",
      "The complete reasoning process:\n",
      "\n",
      "#### Fields\n",
      "- foundation_observations: List of FoundationObservation objects\n",
      "- thoughts: List of Thought objects\n",
      "\n",
      "\n",
      "# Inputs\n",
      "\n",
      "## [Required] Task\n",
      "- The task you are trying to complete\n",
      "\n",
      "## [Optional] Guardrails\n",
      "- A list of guardrails that the user has provided, if any\n",
      "\n",
      "## Output\n",
      "- Reasoning Process\n",
      "    - The full reasoning process objectused to arrive at this output\n",
      "- Findings Summary  \n",
      "    - An in depth summary of your findings\n",
      "- Remaining Questions\n",
      "    - A list of remaining questions or areas for further investigation\n",
      "- Is Conclusion Premature   \n",
      "    - Whether the conclusion is premature or not\n",
      "- Reason for Premature Conclusion\n",
      "    - If is_conclusion_premature is True, contains the reason why. If False, must be the literal string 'conclusion NOT premature'\n",
      "\n",
      "# Process Flow\n",
      "\n",
      "## Foundation Building\n",
      "\n",
      "- Create detailed foundational observations\n",
      "- Explore each observation thoroughly\n",
      "- Revise and refine as needed\n",
      "\n",
      "## Thought Development\n",
      "\n",
      "- Form complete reasoning units\n",
      "- Build on foundational observations\n",
      "- Acknowledge and backtrack from dead ends\n",
      "- Allow for multiple paths of exploration\n",
      "\n",
      "## Conclusion Formation\n",
      "- Create child thoughts from previous reasoning\n",
      "- Evaluate multiple potential conclusions\n",
      "- Continue until reaching a well-supported resolution\n",
      "\n",
      "\n",
      "# Required Inputs and Outputs\n",
      "\n",
      "## Inputs\n",
      "\n",
      "### Required:\n",
      "\n",
      "Task: The problem to analyze\n",
      "\n",
      "### Optional:\n",
      "\n",
      "Guardrails: List of constraints to consider\n",
      "\n",
      "## Outputs\n",
      "\n",
      "- Reasoning Process: Complete analytical framework used\n",
      "- Findings Summary: Detailed analysis results\n",
      "- Remaining Questions: Areas needing further investigation\n",
      "- Is Conclusion Premature: Boolean evaluation\n",
      "- Reason for Premature Conclusion: Explanation if premature, \"conclusion NOT premature\" if complete\n",
      "\n",
      "# Implementation Philosophy and Final Guidance\n",
      "\n",
      "## Core Approach\n",
      "\n",
      "- Begin with first principles and fundamental observations\n",
      "- Question assumptions rigorously at each step\n",
      "- Document all reasoning with clarity and detail\n",
      "- Express thoughts as they naturally emerge and evolve\n",
      "- Embrace uncertainty as a tool for deeper exploration\n",
      "- Persist through multiple attempts and approaches\n",
      "\n",
      "## Process Visualization\n",
      "\n",
      "Think of your analysis as growing and pruning a thought tree:\n",
      "\n",
      "- Each branch represents a line of reasoning\n",
      "- New thoughts expand the tree in multiple directions\n",
      "- Critical evaluation helps prune unnecessary branches\n",
      "- The strongest branches naturally lead to conclusions\n",
      "- The final shape emerges through organic exploration\n",
      "\n",
      "## Final Philosophy\n",
      "\n",
      "Remember that no analytical task is truly impossible - it simply requires:\n",
      "\n",
      "- Breaking complex problems into fundamental components\n",
      "- Exploring multiple novel approaches\n",
      "- Maintaining determined persistence\n",
      "- Allowing thorough contemplation to guide the way\n",
      "- Building confidence through exhaustive exploration\n",
      "\n",
      "The goal isn't just to reach a conclusion, but to arrive there through natural, thorough exploration that leaves no stone unturned. \n",
      "When you reach your conclusion, you should feel confident that your thought process has led you there organically and inevitably.\n",
      "\"\"\"\n",
      "\n",
      "\n",
      " # Response Model\n",
      "You must respond with a valid JSON object that matches the response model below:\n",
      "<response model>\n",
      "{\n",
      "  \"name\": \"InDepthStructuredReasoning\",\n",
      "  \"description\": \"Correctly extracted `InDepthStructuredReasoning` with all the required parameters with correct types\",\n",
      "  \"parameters\": {\n",
      "    \"$defs\": {\n",
      "      \"AtomicStep\": {\n",
      "        \"properties\": {\n",
      "          \"key\": {\n",
      "            \"description\": \"Key of the atomic step. Must have format of AS_<number> (ex: AS_1, AS_2, etc.)\",\n",
      "            \"title\": \"Key\",\n",
      "            \"type\": \"string\"\n",
      "          },\n",
      "          \"previous_step_key\": {\n",
      "            \"anyOf\": [\n",
      "              {\n",
      "                \"type\": \"string\"\n",
      "              },\n",
      "              {\n",
      "                \"const\": \"None\",\n",
      "                \"enum\": [\n",
      "                  \"None\"\n",
      "                ],\n",
      "                \"type\": \"string\"\n",
      "              }\n",
      "            ],\n",
      "            \"description\": \"Key of the previous atomic step (ex: AS_1, AS_2, etc.). If this is the first step, must be the literal string 'None'.\",\n",
      "            \"title\": \"Previous Step Key\"\n",
      "          },\n",
      "          \"content\": {\n",
      "            \"anyOf\": [\n",
      "              {\n",
      "                \"type\": \"string\"\n",
      "              },\n",
      "              {\n",
      "                \"const\": \"DEAD_END\",\n",
      "                \"enum\": [\n",
      "                  \"DEAD_END\"\n",
      "                ],\n",
      "                \"type\": \"string\"\n",
      "              }\n",
      "            ],\n",
      "            \"description\": \"Use short, simple sentences that mirror natural thought patterns\",\n",
      "            \"title\": \"Content\"\n",
      "          }\n",
      "        },\n",
      "        \"required\": [\n",
      "          \"key\",\n",
      "          \"previous_step_key\",\n",
      "          \"content\"\n",
      "        ],\n",
      "        \"title\": \"AtomicStep\",\n",
      "        \"type\": \"object\"\n",
      "      },\n",
      "      \"FoundationObservation\": {\n",
      "        \"properties\": {\n",
      "          \"key\": {\n",
      "            \"description\": \"Key of the foundation observation. Must have format of FO_<number> (ex: FO_1, FO_2, etc.)\",\n",
      "            \"title\": \"Key\",\n",
      "            \"type\": \"string\"\n",
      "          },\n",
      "          \"name\": {\n",
      "            \"description\": \"General name of the foundation observation.\",\n",
      "            \"title\": \"Name\",\n",
      "            \"type\": \"string\"\n",
      "          },\n",
      "          \"atomic_steps\": {\n",
      "            \"description\": \"List of atomic steps that make up the foundation observation.\",\n",
      "            \"items\": {\n",
      "              \"$ref\": \"#/$defs/AtomicStep\"\n",
      "            },\n",
      "            \"title\": \"Atomic Steps\",\n",
      "            \"type\": \"array\"\n",
      "          }\n",
      "        },\n",
      "        \"required\": [\n",
      "          \"key\",\n",
      "          \"name\",\n",
      "          \"atomic_steps\"\n",
      "        ],\n",
      "        \"title\": \"FoundationObservation\",\n",
      "        \"type\": \"object\"\n",
      "      },\n",
      "      \"GuardRailEnum\": {\n",
      "        \"enum\": [\n",
      "          \"Numerical Stability\",\n",
      "          \"Memory Usage\",\n",
      "          \"Batch Handling\",\n",
      "          \"Performance\",\n",
      "          \"Dependencies\"\n",
      "        ],\n",
      "        \"title\": \"GuardRailEnum\",\n",
      "        \"type\": \"string\"\n",
      "      },\n",
      "      \"ReasoningProcess\": {\n",
      "        \"properties\": {\n",
      "          \"foundation_observations\": {\n",
      "            \"description\": \"List of foundation observations that make up the reasoning process.\",\n",
      "            \"items\": {\n",
      "              \"$ref\": \"#/$defs/FoundationObservation\"\n",
      "            },\n",
      "            \"title\": \"Foundation Observations\",\n",
      "            \"type\": \"array\"\n",
      "          },\n",
      "          \"thoughts\": {\n",
      "            \"description\": \"List of thoughts that make up the reasoning process.\",\n",
      "            \"items\": {\n",
      "              \"$ref\": \"#/$defs/Thought\"\n",
      "            },\n",
      "            \"title\": \"Thoughts\",\n",
      "            \"type\": \"array\"\n",
      "          }\n",
      "        },\n",
      "        \"required\": [\n",
      "          \"foundation_observations\",\n",
      "          \"thoughts\"\n",
      "        ],\n",
      "        \"title\": \"ReasoningProcess\",\n",
      "        \"type\": \"object\"\n",
      "      },\n",
      "      \"Thought\": {\n",
      "        \"properties\": {\n",
      "          \"key\": {\n",
      "            \"description\": \"Key of the thought. Must have format of TH_<number> (ex: TH_1, TH_2, etc.)\",\n",
      "            \"title\": \"Key\",\n",
      "            \"type\": \"string\"\n",
      "          },\n",
      "          \"backtracked_from\": {\n",
      "            \"anyOf\": [\n",
      "              {\n",
      "                \"type\": \"string\"\n",
      "              },\n",
      "              {\n",
      "                \"const\": \"None\",\n",
      "                \"enum\": [\n",
      "                  \"None\"\n",
      "                ],\n",
      "                \"type\": \"string\"\n",
      "              }\n",
      "            ],\n",
      "            \"description\": \"If the thought is a backtrack, the key of the thought it was backtracked from (ex: TH_1, TH_2, etc.). If not, must be the literal string 'None'.\",\n",
      "            \"title\": \"Backtracked From\"\n",
      "          },\n",
      "          \"parent_thought\": {\n",
      "            \"anyOf\": [\n",
      "              {\n",
      "                \"type\": \"string\"\n",
      "              },\n",
      "              {\n",
      "                \"const\": \"None\",\n",
      "                \"enum\": [\n",
      "                  \"None\"\n",
      "                ],\n",
      "                \"type\": \"string\"\n",
      "              }\n",
      "            ],\n",
      "            \"description\": \"If the thought is a child thought of a previous thought, the key of the thought this thought was born from (ex: TH_1, TH_2, etc.). If not, must be the literal string 'None'.\",\n",
      "            \"title\": \"Parent Thought\"\n",
      "          },\n",
      "          \"associated_foundation_observations\": {\n",
      "            \"description\": \"List of keys of the foundation observations that this thought is associated with (ex: FO_1, FO_2, etc.).\",\n",
      "            \"items\": {\n",
      "              \"type\": \"string\"\n",
      "            },\n",
      "            \"title\": \"Associated Foundation Observations\",\n",
      "            \"type\": \"array\"\n",
      "          },\n",
      "          \"name\": {\n",
      "            \"description\": \"Use short, simple sentences that mirror natural thought patterns\",\n",
      "            \"title\": \"Name\",\n",
      "            \"type\": \"string\"\n",
      "          },\n",
      "          \"guard_rails_to_consider\": {\n",
      "            \"description\": \"Guard rails to consider at this step.\",\n",
      "            \"items\": {\n",
      "              \"$ref\": \"#/$defs/GuardRailEnum\"\n",
      "            },\n",
      "            \"title\": \"Guard Rails To Consider\",\n",
      "            \"type\": \"array\"\n",
      "          },\n",
      "          \"thought_process\": {\n",
      "            \"description\": \"List of atomic steps that make up the thought.\",\n",
      "            \"items\": {\n",
      "              \"$ref\": \"#/$defs/AtomicStep\"\n",
      "            },\n",
      "            \"title\": \"Thought Process\",\n",
      "            \"type\": \"array\"\n",
      "          }\n",
      "        },\n",
      "        \"required\": [\n",
      "          \"key\",\n",
      "          \"backtracked_from\",\n",
      "          \"parent_thought\",\n",
      "          \"associated_foundation_observations\",\n",
      "          \"name\",\n",
      "          \"guard_rails_to_consider\",\n",
      "          \"thought_process\"\n",
      "        ],\n",
      "        \"title\": \"Thought\",\n",
      "        \"type\": \"object\"\n",
      "      }\n",
      "    },\n",
      "    \"properties\": {\n",
      "      \"reasoning_process\": {\n",
      "        \"allOf\": [\n",
      "          {\n",
      "            \"$ref\": \"#/$defs/ReasoningProcess\"\n",
      "          }\n",
      "        ],\n",
      "        \"description\": \"The full, in-depth reasoning process used to arrive at this output.\"\n",
      "      },\n",
      "      \"findings_summary\": {\n",
      "        \"description\": \"An in depth summary of your findings.\",\n",
      "        \"title\": \"Findings Summary\",\n",
      "        \"type\": \"string\"\n",
      "      },\n",
      "      \"remaining_questions\": {\n",
      "        \"description\": \"A list of remaining questions or areas for further investigation.\",\n",
      "        \"items\": {\n",
      "          \"type\": \"string\"\n",
      "        },\n",
      "        \"title\": \"Remaining Questions\",\n",
      "        \"type\": \"array\"\n",
      "      },\n",
      "      \"is_conclusion_premature\": {\n",
      "        \"description\": \"Whether the conclusion is premature or not.\",\n",
      "        \"title\": \"Is Conclusion Premature\",\n",
      "        \"type\": \"boolean\"\n",
      "      },\n",
      "      \"reason_for_premature_conclusion\": {\n",
      "        \"anyOf\": [\n",
      "          {\n",
      "            \"const\": \"conclusion NOT premature\",\n",
      "            \"enum\": [\n",
      "              \"conclusion NOT premature\"\n",
      "            ],\n",
      "            \"type\": \"string\"\n",
      "          },\n",
      "          {\n",
      "            \"type\": \"string\"\n",
      "          }\n",
      "        ],\n",
      "        \"description\": \"If is_conclusion_premature is True, contains the reason why. If False, must be the literal string 'conclusion NOT premature'.\",\n",
      "        \"title\": \"Reason For Premature Conclusion\"\n",
      "      }\n",
      "    },\n",
      "    \"required\": [\n",
      "      \"findings_summary\",\n",
      "      \"is_conclusion_premature\",\n",
      "      \"reason_for_premature_conclusion\",\n",
      "      \"reasoning_process\",\n",
      "      \"remaining_questions\"\n",
      "    ],\n",
      "    \"type\": \"object\"\n",
      "  }\n",
      "}\n",
      "</response model>\n",
      "\n",
      "</system prompt>\n",
      "\n",
      "\n",
      "<user prompt>\n",
      " Please provide a detailed step by step reasoning process for the following task:\n",
      "# Task: \n",
      "Design an optimized CUDA kernel implementation for softmax that maximizes throughput while maintaining numerical stability\n",
      "\n",
      "# Guardrails:\n",
      "{\n",
      "  \"name\": \"Numerical Stability\",\n",
      "  \"description\": \"Must maintain numerical stability (handling overflow/underflow)\"\n",
      "}\n",
      "{\n",
      "  \"name\": \"Memory Usage\",\n",
      "  \"description\": \"Maximum shared memory usage of 48KB per block\"\n",
      "}\n",
      "{\n",
      "  \"name\": \"Batch Handling\",\n",
      "  \"description\": \"Must handle variable batch sizes efficiently\"\n",
      "}\n",
      "{\n",
      "  \"name\": \"Performance\",\n",
      "  \"description\": \"Must outperform naive implementation by at least 100x\"\n",
      "}\n",
      "{\n",
      "  \"name\": \"Dependencies\",\n",
      "  \"description\": \"Cannot use external CUDA libraries (only basic CUDA primitives)\"\n",
      "}\n",
      "</user prompt>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(make_gpt_pro_prompt(IN_DEPTH_THINKING_SYSTEM_PROMPT, TASK, GUARDRAILS, GuardRailEnum))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading GPT Pro Response\n",
    "\n",
    "GPT Pro returns responses in its own JSON format that requires some processing before use. Here's how to handle the responses:\n",
    "\n",
    "1. Save the GPT Pro response as a JSON file in your project directory\n",
    "2. Load and parse the JSON file \n",
    "3. Extract the relevant data from the response structure\n",
    "4. Pass the extracted data to the Mermaid diagram builder\n",
    "\n",
    "The example below demonstrates loading a saved response and generating a diagram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'is_gpt_prompt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/outputs/cuda_softmax/gpt_pro/response_1.json\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      2\u001b[0m     gpt_pro_response \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m----> 4\u001b[0m diagram_text \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_mermaid_diagram\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgpt_pro_response\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mparameters\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_gpt_pro\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(diagram_text)\n",
      "File \u001b[0;32m~/Github Repos/Lascari AI/DAGofThought/utils/build_mermaid_diagram.py:99\u001b[0m, in \u001b[0;36mbuild_mermaid_diagram\u001b[0;34m(data, is_gpt_pro)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, gr \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(guard_rails, start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 99\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mis_gpt_prompt\u001b[49m:\n\u001b[1;32m    100\u001b[0m             gr_value \u001b[38;5;241m=\u001b[39m gr\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    101\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'is_gpt_prompt' is not defined"
     ]
    }
   ],
   "source": [
    "with open('../data/outputs/cuda_softmax/gpt_pro/response_1.json', 'r') as f:\n",
    "    gpt_pro_response = json.load(f)\n",
    "    \n",
    "diagram_text = build_mermaid_diagram(gpt_pro_response['parameters'], is_gpt_pro=True)\n",
    "print(diagram_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "promo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
